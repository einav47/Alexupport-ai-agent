<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/.env">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/.env" />
              <option name="originalContent" value="# ==== Azure OpenAI ====&#10;AZURE_OPENAI_API_KEY=&quot;8HIrkiHT0KYgwTOXUYb7PjXnQAo7x7lk6mxy0BXSQAh9hNbpVOVqJQQJ99AKACYeBjFXJ3w3AAABACOG7iAn&quot;  # Replace with your Azure OpenAI API key&#10;AZURE_OPENAI_ENDPOINT=https://096290-oai.openai.azure.com  # Replace with your Azure OpenAI endpoint&#10;AZURE_OPENAI_API_VERSION=2023-05-15&#10;AZURE_OPENAI_CHAT_DEPLOYMENT=team13-gpt4o&#10;AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT=team13-embedding&#10;OPENAI_CHAT_MODEL=gpt-4o&#10;OPENAI_EMBEDDINGS_MODEL=gpt-4o&#10;&#10;# ==== Qdrant ====&#10;QDRANT_URL=https://63ad19dc-7779-4868-bc81-41f5fae4353a.europe-west3-0.gcp.cloud.qdrant.io:6333  # Replace with your Qdrant URL&#10;QDRANT_API_KEY=&quot;eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0._mnughDNgXpg2I_tMDwpIIKZJiDqma2o_YDld0ZseR4&quot;  # Replace with your Qdrant API key&#10;QDRANT_COLLECTION=data_collection  # Ensure this matches your Qdrant collection name&#10;QDRANT_VECTOR_NAME=questionText  # Ensure this matches your Qdrant vector name" />
              <option name="updatedContent" value="# ==== Azure OpenAI ====&#10;AZURE_OPENAI_API_KEY=&quot;8HIrkiHT0KYgwTOXUYb7PjXnQAo7x7lk6mxy0BXSQAh9hNbpVOVqJQQJ99AKACYeBjFXJ3w3AAABACOG7iAn&quot;  # Replace with your Azure OpenAI API key&#10;AZURE_OPENAI_ENDPOINT=https://096290-oai.openai.azure.com  # Replace with your Azure OpenAI endpoint&#10;AZURE_OPENAI_API_VERSION=2023-05-15&#10;AZURE_OPENAI_CHAT_DEPLOYMENT=team13-gpt4o&#10;AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT=team13-embedding&#10;OPENAI_CHAT_MODEL=gpt-4o&#10;OPENAI_EMBEDDINGS_MODEL=gpt-4o&#10;&#10;# ==== Qdrant ====&#10;QDRANT_URL=https://63ad19dc-7779-4868-bc81-41f5fae4353a.europe-west3-0.gcp.cloud.qdrant.io:6333  # Replace with your Qdrant URL&#10;QDRANT_API_KEY=&quot;eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0._mnughDNgXpg2I_tMDwpIIKZJiDqma2o_YDld0ZseR4&quot;  # Replace with your Qdrant API key&#10;QDRANT_COLLECTION=data_collection  # Ensure this matches your Qdrant collection name&#10;QDRANT_VECTOR_NAME=questionText  # Ensure this matches your Qdrant vector name" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/agent/alexupport_agent.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/agent/alexupport_agent.py" />
              <option name="originalContent" value="&quot;&quot;&quot;&#10;Main Alexupport agent&#10;This module contains the main agent which orchestrates the other microagents&#10;&quot;&quot;&quot;&#10;&#10;from typing import List&#10;&#10;from langchain.memory import ConversationBufferMemory&#10;from langchain.schema import HumanMessage, AIMessage&#10;&#10;from agent.answer_generator import AnswerGenerator&#10;from agent.followup_generator import FollowUpGenerator&#10;from agent.input_refiner import InputRefiner&#10;from agent.is_answerable_agent import IsAnswerableAgent&#10;from agent.is_relevant_agent import IsRelevantAgent&#10;from agent.information_retriever import InformationRetriever, client&#10;from agent.llm_client import LLMClient&#10;&#10;LLM_CLIENT = LLMClient()&#10;&#10;class AlexupportAgent:&#10;    &quot;&quot;&quot;Main Alexupport agent class&quot;&quot;&quot;&#10;&#10;    def __init__(self):&#10;&#10;        self.memory = ConversationBufferMemory(return_messages=True)&#10;&#10;        # Initializing all microagents&#10;        self.input_refiner = InputRefiner(llm_client=LLM_CLIENT)&#10;        self.information_retriever = InformationRetriever(qdrant_client=client, llm_client=LLM_CLIENT)&#10;        self.is_answerable_agent = IsAnswerableAgent(llm_client=LLM_CLIENT)&#10;        self.answer_generator = AnswerGenerator(llm_client=LLM_CLIENT, chat_history=self.memory)&#10;        self.followup_generator = FollowUpGenerator(llm_client=LLM_CLIENT)&#10;        self.is_relevant_generator = IsRelevantAgent(llm_client=LLM_CLIENT)&#10;&#10;    def intro(self) -&gt; str:&#10;        return (&quot;Hi! I’m **Alexupport**  — your Amazon product assistant.\n\n&quot;&#10;                &quot;Pick a product on the left, then ask me anything about it! I will do my best to help &quot;)&#10;&#10;    def format_final_answer(self, answer: str, follow_ups: List[str]) -&gt; str:&#10;        &quot;&quot;&quot;&#10;        Formats the final answer with follow-up questions.&#10;&#10;        Parameters:&#10;        - answer: str; The generated answer to the user's query.&#10;        - follow_ups: List[str]; A list of follow-up questions related to the user's query.&#10;&#10;        Returns:&#10;        - str; The formatted final answer with follow-up questions.&#10;        &quot;&quot;&quot;&#10;&#10;        formatted_answer = f&quot;&quot;&quot;&#10;        {answer}&#10;        Here are some follow-up questions you might consider:&#10;        {&quot;; &quot;.join([f for f in follow_ups])}&#10;        &quot;&quot;&quot;&#10;&#10;        return formatted_answer&#10;&#10;    def answer(self, user_query: str, asin: str) -&gt; str:&#10;        try:&#10;            # Step 0 - Append the user query&#10;            self.memory.chat_memory.messages.append(HumanMessage(content=user_query))&#10;&#10;            # Step 1 - Refine user query&#10;            refined_query = self.input_refiner.refine_input(user_input=user_query)&#10;&#10;            # Step 2 - Retrieve relevant information from the DB&#10;            retrieved_info = self.information_retriever.retrieve_information(&#10;                query=refined_query,&#10;                product_id=asin&#10;            )&#10;&#10;            # Step 3 - Check if the query is answerable&#10;            answerable_result = self.is_answerable_agent.check_answerability(&#10;                user_question=refined_query,&#10;                retrieved_info=retrieved_info&#10;            )&#10;            print(f&quot;DEBUG: check_answerability result: {answerable_result}&quot;)  # Debug log&#10;&#10;            # Ensure the result is unpacked correctly&#10;            if not isinstance(answerable_result, tuple) or len(answerable_result) != 2:&#10;                raise ValueError(&quot;check_answerability must return a tuple (bool, str).&quot;)&#10;&#10;            answerable, reason1 = answerable_result&#10;&#10;            if not answerable:&#10;                msg = f&quot;Sorry — I couldn't find enough reliable info to answer that.\n\nReason: {reason1}&quot;&#10;&#10;                followup_questions = self.followup_generator.generate_follow_ups(&#10;                    user_question=refined_query,&#10;                    context=retrieved_info&#10;                )&#10;&#10;                final_answer = self.format_final_answer(msg, followup_questions)&#10;                self.memory.chat_memory.messages.append(AIMessage(content=final_answer))&#10;                return final_answer&#10;&#10;            iteration = 0&#10;&#10;            while iteration &lt; 5:&#10;                # Step 4 - Generate answer&#10;                answer = self.answer_generator.generate_answer(&#10;                    user_question=refined_query,&#10;                    context=retrieved_info&#10;                )&#10;&#10;                # Step 5 - Check answer relevance&#10;                relevant, reason2 = self.is_relevant_generator.assess_relevance(&#10;                    user_question=refined_query,&#10;                    generated_response=answer,&#10;                    context=retrieved_info&#10;                )&#10;&#10;                if relevant:&#10;                    # Step 6 - Generate follow-up questions&#10;                    followup_questions = self.followup_generator.generate_follow_ups(&#10;                        user_question=refined_query,&#10;                        context=retrieved_info&#10;                    )&#10;&#10;                    final_answer = self.format_final_answer(&#10;                        answer=answer,&#10;                        follow_ups=followup_questions&#10;                    )&#10;&#10;                    self.memory.chat_memory.messages.append(AIMessage(content=final_answer))&#10;                    return final_answer&#10;&#10;                iteration += 1&#10;&#10;            msg = f&quot;Sorry — I couldn't find a relevant answer to that.\n\nReason: {reason2}&quot;&#10;&#10;            followup_questions = self.followup_generator.generate_follow_ups(&#10;                user_question=refined_query,&#10;                context=retrieved_info&#10;            )&#10;&#10;            final_answer = self.format_final_answer(msg, followup_questions)&#10;            self.memory.chat_memory.messages.append(AIMessage(content=final_answer))&#10;            return final_answer&#10;&#10;        except Exception as e:&#10;            error_message = f&quot;An error occurred while processing your request: {e}&quot;&#10;            self.memory.chat_memory.messages.append(AIMessage(content=error_message))&#10;            return error_message&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Main Alexupport agent&#10;This module contains the main agent which orchestrates the other microagents&#10;&quot;&quot;&quot;&#10;&#10;from typing import List&#10;&#10;from langchain.memory import ConversationBufferMemory&#10;from langchain.schema import HumanMessage, AIMessage&#10;&#10;from agent.answer_generator import AnswerGenerator&#10;from agent.followup_generator import FollowUpGenerator&#10;from agent.input_refiner import InputRefiner&#10;from agent.is_answerable_agent import IsAnswerableAgent&#10;from agent.is_relevant_agent import IsRelevantAgent&#10;from agent.information_retriever import InformationRetriever, client&#10;from agent.llm_client import LLMClient&#10;&#10;LLM_CLIENT = LLMClient()&#10;&#10;class AlexupportAgent:&#10;    &quot;&quot;&quot;Main Alexupport agent class&quot;&quot;&quot;&#10;&#10;    def __init__(self):&#10;&#10;        self.memory = ConversationBufferMemory(return_messages=True)&#10;&#10;        # Initializing all microagents&#10;        self.input_refiner = InputRefiner(llm_client=LLM_CLIENT)&#10;        self.information_retriever = InformationRetriever(qdrant_client=client, llm_client=LLM_CLIENT)&#10;        self.is_answerable_agent = IsAnswerableAgent(llm_client=LLM_CLIENT)&#10;        self.answer_generator = AnswerGenerator(llm_client=LLM_CLIENT, chat_history=self.memory)&#10;        self.followup_generator = FollowUpGenerator(llm_client=LLM_CLIENT)&#10;        self.is_relevant_generator = IsRelevantAgent(llm_client=LLM_CLIENT)&#10;&#10;    def intro(self) -&gt; str:&#10;        return (&quot;Hi! I’m **Alexupport**  — your Amazon product assistant.\n\n&quot;&#10;                &quot;Pick a product on the left, then ask me anything about it! I will do my best to help &quot;)&#10;&#10;    def format_final_answer(self, answer: str, follow_ups: List[str]) -&gt; str:&#10;        &quot;&quot;&quot;&#10;        Formats the final answer with follow-up questions.&#10;&#10;        Parameters:&#10;        - answer: str; The generated answer to the user's query.&#10;        - follow_ups: List[str]; A list of follow-up questions related to the user's query.&#10;&#10;        Returns:&#10;        - str; The formatted final answer with follow-up questions.&#10;        &quot;&quot;&quot;&#10;&#10;        formatted_answer = f&quot;&quot;&quot;&#10;        {answer}&#10;        Here are some follow-up questions you might consider:&#10;        {&quot;; &quot;.join([f for f in follow_ups])}&#10;        &quot;&quot;&quot;&#10;&#10;        return formatted_answer&#10;&#10;    def answer(self, user_query: str, asin: str) -&gt; str:&#10;        try:&#10;            # Step 0 - Append the user query&#10;            self.memory.chat_memory.messages.append(HumanMessage(content=user_query))&#10;&#10;            # Step 1 - Refine user query&#10;            refined_query = self.input_refiner.refine_input(user_input=user_query)&#10;&#10;            # Step 2 - Retrieve relevant information from the DB&#10;            retrieved_info = self.information_retriever.retrieve_information(&#10;                query=refined_query,&#10;                product_id=asin&#10;            )&#10;&#10;            # Step 3 - Check if the query is answerable&#10;            answerable_result = self.is_answerable_agent.check_answerability(&#10;                user_question=refined_query,&#10;                retrieved_info=retrieved_info&#10;            )&#10;            print(f&quot;DEBUG: check_answerability result: {answerable_result}&quot;)  # Debug log&#10;&#10;            # Ensure the result is unpacked correctly&#10;            if not isinstance(answerable_result, tuple) or len(answerable_result) != 2:&#10;                raise ValueError(&quot;check_answerability must return a tuple (bool, str).&quot;)&#10;&#10;            answerable, reason1 = answerable_result&#10;&#10;            if not answerable:&#10;                msg = f&quot;Sorry — I couldn't find enough reliable info to answer that.\n\nReason: {reason1}&quot;&#10;&#10;                followup_questions = self.followup_generator.generate_follow_ups(&#10;                    user_question=refined_query,&#10;                    context=retrieved_info&#10;                )&#10;&#10;                final_answer = self.format_final_answer(msg, followup_questions)&#10;                self.memory.chat_memory.messages.append(AIMessage(content=final_answer))&#10;                return final_answer&#10;&#10;            iteration = 0&#10;&#10;            while iteration &lt; 5:&#10;                # Step 4 - Generate answer&#10;                answer = self.answer_generator.generate_answer(&#10;                    user_question=refined_query,&#10;                    context=retrieved_info&#10;                )&#10;&#10;                # Step 5 - Check answer relevance&#10;                relevance_result = self.is_relevant_generator.assess_relevance(&#10;                    user_question=refined_query,&#10;                    generated_response=answer,&#10;                    context=retrieved_info&#10;                )&#10;                print(f&quot;DEBUG: assess_relevance result: {relevance_result}&quot;)  # Debug log&#10;&#10;                # Ensure the result is unpacked correctly&#10;                if not isinstance(relevance_result, tuple) or len(relevance_result) != 2:&#10;                    raise ValueError(&quot;assess_relevance must return a tuple (bool, str).&quot;)&#10;&#10;                relevant, reason2 = relevance_result&#10;&#10;                if relevant:&#10;                    # Step 6 - Generate follow-up questions&#10;                    followup_questions = self.followup_generator.generate_follow_ups(&#10;                        user_question=refined_query,&#10;                        context=retrieved_info&#10;                    )&#10;&#10;                    final_answer = self.format_final_answer(&#10;                        answer=answer,&#10;                        follow_ups=followup_questions&#10;                    )&#10;&#10;                    self.memory.chat_memory.messages.append(AIMessage(content=final_answer))&#10;                    return final_answer&#10;&#10;                iteration += 1&#10;&#10;            msg = f&quot;Sorry — I couldn't find a relevant answer to that.\n\nReason: {reason2}&quot;&#10;&#10;            followup_questions = self.followup_generator.generate_follow_ups(&#10;                user_question=refined_query,&#10;                context=retrieved_info&#10;            )&#10;&#10;            final_answer = self.format_final_answer(msg, followup_questions)&#10;            self.memory.chat_memory.messages.append(AIMessage(content=final_answer))&#10;            return final_answer&#10;&#10;        except Exception as e:&#10;            error_message = f&quot;An error occurred while processing your request: {e}&quot;&#10;            self.memory.chat_memory.messages.append(AIMessage(content=error_message))&#10;            return error_message" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/agent/information_retriever.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/agent/information_retriever.py" />
              <option name="originalContent" value="&quot;&quot;&quot;&#10;This module defines the InformationRetriever microagent,&#10;which is responsible for retrieveing relevant information from the Qdrant database.&#10;&quot;&quot;&quot;&#10;&#10;from qdrant_client import QdrantClient&#10;from qdrant_client.models import Filter, FieldCondition, MatchValue, PayloadSelector&#10;&#10;from agent.llm_client import LLMClient&#10;&#10;QDRANT_URL = &quot;https://63ad19dc-7779-4868-bc81-41f5fae4353a.europe-west3-0.gcp.cloud.qdrant.io:6333&quot;&#10;QDRANT_API_KEY = &quot;eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0._mnughDNgXpg2I_tMDwpIIKZJiDqma2o_YDld0ZseR4&quot;&#10;client = QdrantClient(&#10;    url=QDRANT_URL,&#10;    api_key=QDRANT_API_KEY&#10;)&#10;&#10;class InformationRetriever:&#10;    &quot;&quot;&quot;Microagent class for retrieving relevant information from the Qdrant database&quot;&quot;&quot;&#10;&#10;    def __init__(self, qdrant_client: QdrantClient, llm_client: LLMClient):&#10;        self.qdrant_client = qdrant_client&#10;        self.llm_client = llm_client&#10;&#10;    def list_products(self, limit: int = 500) -&gt; list[dict]:&#10;        &quot;&quot;&quot;Return [{'asin','productTitle'}] found in collection payloads.&quot;&quot;&quot;&#10;        collected, page_offset, seen = [], None, set()&#10;        while len(collected) &lt; limit:&#10;            resp = self.qdrant_client.scroll(&#10;                collection_name=&quot;data_collection&quot;,&#10;                with_payload=True,&#10;                with_vectors=False,&#10;                limit=256,&#10;                offset=page_offset&#10;            )&#10;            print(&quot;DEBUG:&quot;, type(resp), hasattr(resp, &quot;points&quot;))&#10;&#10;            # Handle both tuple and object responses&#10;            if hasattr(resp, &quot;points&quot;):  # ScrollResult style&#10;                points = resp.points&#10;                page_offset = getattr(resp, &quot;next_page_offset&quot;, None)&#10;            else:  # legacy tuple style: (points, next_page_offset) or (points, next_page_offset, has_more)&#10;                points = resp[0]&#10;                page_offset = resp[1] if len(resp) &gt; 1 else None&#10;&#10;            if not points:&#10;                break&#10;&#10;            for p in points:&#10;                payload = getattr(p, &quot;payload&quot;, {}) or {}&#10;                asin = payload.get(&quot;asin&quot;)&#10;                title = payload.get(&quot;productTitle&quot;)&#10;                if asin and asin not in seen:&#10;                    seen.add(asin)&#10;                    collected.append({&quot;asin&quot;: asin, &quot;productTitle&quot;: title or &quot;(untitled)&quot;})&#10;&#10;            if page_offset is None:&#10;                break&#10;&#10;        return collected&#10;&#10;    def retrieve_information(self, query: str, product_id: str) -&gt; list:&#10;        &quot;&quot;&quot;&#10;        Retrieve relevant information from the Qdrant database based on the user's query.&#10;&#10;        Parameters:&#10;        - query (str): The user's question or query.&#10;        - product_id (str): The product ID (ASIN) to filter results.&#10;&#10;        Returns:&#10;        - list: A list of relevant documents or snippets related to the query and product ID.&#10;        &quot;&quot;&quot;&#10;&#10;        try:&#10;            query_vector = self.llm_client.generate_embeddings(texts=[query])&#10;&#10;            # Use the Qdrant client to perform the search&#10;            search_results = self.qdrant_client.query_points(&#10;                collection_name=&quot;data_collection&quot;,&#10;                query=query_vector,&#10;                using=&quot;questionText&quot;,&#10;                limit=10,&#10;                with_payload=True,  # Ensure payloads are included&#10;                query_filter=Filter(&#10;                    must=[&#10;                        FieldCondition(key=&quot;asin&quot;, match=MatchValue(value=product_id))&#10;                    ]&#10;                )&#10;            )&#10;&#10;            # Keep only the points with a similarity score of at least 0.5&#10;            search_results.points = [point for point in search_results.points if point.score &gt;= 0.5]&#10;&#10;            # Extract relevant fields from the payloads&#10;            results_answers_and_review_snippets = [&#10;                result.payload.get('answers', []) + result.payload.get('review_snippets', [])&#10;                for result in search_results.points&#10;            ]&#10;            return results_answers_and_review_snippets&#10;&#10;        except Exception as e:&#10;            raise RuntimeError(f&quot;Error retrieving information: {e}&quot;)&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;This module defines the InformationRetriever microagent,&#10;which is responsible for retrieveing relevant information from the Qdrant database.&#10;&quot;&quot;&quot;&#10;&#10;from qdrant_client import QdrantClient&#10;from qdrant_client.models import Filter, FieldCondition, MatchValue, PayloadSelector&#10;&#10;from agent.llm_client import LLMClient&#10;&#10;QDRANT_URL = &quot;https://63ad19dc-7779-4868-bc81-41f5fae4353a.europe-west3-0.gcp.cloud.qdrant.io:6333&quot;&#10;QDRANT_API_KEY = &quot;eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0._mnughDNgXpg2I_tMDwpIIKZJiDqma2o_YDld0ZseR4&quot;&#10;client = QdrantClient(&#10;    url=QDRANT_URL,&#10;    api_key=QDRANT_API_KEY&#10;)&#10;&#10;class InformationRetriever:&#10;    &quot;&quot;&quot;Microagent class for retrieving relevant information from the Qdrant database&quot;&quot;&quot;&#10;&#10;    def __init__(self, qdrant_client: QdrantClient, llm_client: LLMClient):&#10;        self.qdrant_client = qdrant_client&#10;        self.llm_client = llm_client&#10;&#10;    def list_products(self, limit: int = 500) -&gt; list[dict]:&#10;        &quot;&quot;&quot;Return [{'asin','productTitle'}] found in collection payloads.&quot;&quot;&quot;&#10;        collected, page_offset, seen = [], None, set()&#10;        while len(collected) &lt; limit:&#10;            resp = self.qdrant_client.scroll(&#10;                collection_name=&quot;data_collection&quot;,&#10;                with_payload=True,&#10;                with_vectors=False,&#10;                limit=256,&#10;                offset=page_offset&#10;            )&#10;            print(&quot;DEBUG:&quot;, type(resp), hasattr(resp, &quot;points&quot;))&#10;&#10;            # Handle both tuple and object responses&#10;            if hasattr(resp, &quot;points&quot;):  # ScrollResult style&#10;                points = resp.points&#10;                page_offset = getattr(resp, &quot;next_page_offset&quot;, None)&#10;            else:  # legacy tuple style: (points, next_page_offset) or (points, next_page_offset, has_more)&#10;                points = resp[0]&#10;                page_offset = resp[1] if len(resp) &gt; 1 else None&#10;&#10;            if not points:&#10;                break&#10;&#10;            for p in points:&#10;                payload = getattr(p, &quot;payload&quot;, {}) or {}&#10;                asin = payload.get(&quot;asin&quot;)&#10;                title = payload.get(&quot;productTitle&quot;)&#10;                if asin and asin not in seen:&#10;                    seen.add(asin)&#10;                    collected.append({&quot;asin&quot;: asin, &quot;productTitle&quot;: title or &quot;(untitled)&quot;})&#10;&#10;            if page_offset is None:&#10;                break&#10;&#10;        return collected&#10;&#10;    def retrieve_information(self, query: str, product_id: str) -&gt; list:&#10;        &quot;&quot;&quot;&#10;        Retrieve relevant information from the Qdrant database based on the user's query.&#10;&#10;        Parameters:&#10;        - query (str): The user's question or query.&#10;        - product_id (str): The product ID (ASIN) to filter results.&#10;&#10;        Returns:&#10;        - list: A list of relevant documents or snippets related to the query and product ID.&#10;        &quot;&quot;&quot;&#10;&#10;        try:&#10;            # Generate query embeddings&#10;            query_vector = self.llm_client.generate_embeddings(texts=[query])&#10;&#10;            # Perform a semantic search in Qdrant&#10;            search_results = self.qdrant_client.search(&#10;                collection_name=&quot;data_collection&quot;,&#10;                query_vector=query_vector[0],&#10;                limit=10,&#10;                with_payload=True,&#10;                filter=Filter(&#10;                    must=[&#10;                        FieldCondition(key=&quot;asin&quot;, match=MatchValue(value=product_id))&#10;                    ]&#10;                )&#10;            )&#10;&#10;            # Extract relevant fields from the payloads&#10;            results_answers_and_review_snippets = [&#10;                result.payload.get('answers', []) + result.payload.get('review_snippets', [])&#10;                for result in search_results&#10;            ]&#10;            return results_answers_and_review_snippets&#10;&#10;        except Exception as e:&#10;            raise RuntimeError(f&quot;Error retrieving information: {e}&quot;)" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/agent/input_refiner.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/agent/input_refiner.py" />
              <option name="originalContent" value="&quot;&quot;&quot;&#10;This module defines the InputRefiner microagent,&#10;which is responsible for refining and enriching user questions&#10;&quot;&quot;&quot;&#10;&#10;from agent.llm_client import LLMClient&#10;from utils.utils import clean_string&#10;&#10;class InputRefiner:&#10;    &quot;&quot;&quot;Microagent class for refining and enriching user questions&quot;&quot;&quot;&#10;&#10;    def __init__(self, llm_client: LLMClient):&#10;        self.llm_client = llm_client&#10;        self.system_prompt =  &quot;&quot;&quot;&#10;        You are a question refinement specialist for Amazon product support.&#10;&#10;        Given a question from the user, your task is to:&#10;        1. Fix typos and grammatical errors in the user question&#10;        2. Expand abbreviations and clarify ambiguous terms&#10;        3. Maintain the original intent while making the question more specific&#10;&#10;        Return only the refined question without any explanations or additional text.&#10;        &quot;&quot;&quot;&#10;&#10;    def refine_input(self, user_input: str) -&gt; str:&#10;        &quot;&quot;&quot;&#10;        Refines the user input to improve LLM responses.&#10;&#10;        Parameters:&#10;        - user_input: str; The original user question to refine.&#10;&#10;        Returns:&#10;        - str; The refined user question.&#10;        &quot;&quot;&quot;&#10;&#10;        complete_human_input = clean_string(f&quot;&quot;&quot;&#10;        Refine the following user question to improve clarity and specificity.&#10;        Focus on making the question more detailed and relevant to product materials and durability.&#10;&#10;        User Question: {user_input}&#10;        &quot;&quot;&quot;)&#10;&#10;        messages = [&#10;            {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: self.system_prompt},&#10;            {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: complete_human_input}&#10;        ]&#10;&#10;        response = self.llm_client.generate_response(messages=messages)&#10;        return response.strip()&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;This module defines the InputRefiner microagent,&#10;which is responsible for refining and enriching user questions&#10;&quot;&quot;&quot;&#10;&#10;from agent.llm_client import LLMClient&#10;from utils.utils import clean_string&#10;&#10;class InputRefiner:&#10;    &quot;&quot;&quot;Microagent class for refining and enriching user questions&quot;&quot;&quot;&#10;&#10;    def __init__(self, llm_client: LLMClient):&#10;        self.llm_client = llm_client&#10;        self.system_prompt =  &quot;&quot;&quot;&#10;        You are a question refinement specialist for Amazon product support.&#10;&#10;        Given a question from the user, your task is to:&#10;        1. Fix typos and grammatical errors in the user question&#10;        2. Expand abbreviations and clarify ambiguous terms&#10;        3. Maintain the original intent while making the question more specific&#10;&#10;        Return only the refined question without any explanations or additional text.&#10;        &quot;&quot;&quot;&#10;&#10;    def refine_input(self, user_input: str) -&gt; str:&#10;        &quot;&quot;&quot;&#10;        Refines the user input to improve LLM responses.&#10;&#10;        Parameters:&#10;        - user_input: str; The original user question to refine.&#10;&#10;        Returns:&#10;        - str; The refined user question.&#10;        &quot;&quot;&quot;&#10;&#10;        complete_human_input = clean_string(f&quot;&quot;&quot;&#10;        Refine the following user question to improve clarity and specificity.&#10;        Focus on making the question more detailed and relevant to product materials, durability, and other product features.&#10;&#10;        User Question: {user_input}&#10;        &quot;&quot;&quot;)&#10;&#10;        messages = [&#10;            {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: self.system_prompt},&#10;            {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: complete_human_input}&#10;        ]&#10;&#10;        # Generate a refined query&#10;        refined_query = self.llm_client.generate_response(messages=messages).strip()&#10;&#10;        # Add semantic context if necessary&#10;        if &quot;rust&quot; in user_input.lower():&#10;            refined_query += &quot; Is it made of stainless steel or other rust-resistant materials?&quot;&#10;&#10;        return refined_query" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/agent/is_relevant_agent.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/agent/is_relevant_agent.py" />
              <option name="originalContent" value="&quot;&quot;&quot;&#10;This module defines the IsRelevantAgent microagent,&#10;which is responsible for determining the relevance of the generated response&#10;to the user's question.&#10;&quot;&quot;&quot;&#10;&#10;from typing import List&#10;&#10;from agent.llm_client import LLMClient&#10;from utils.utils import clean_string&#10;&#10;class IsRelevantAgent:&#10;    &quot;&quot;&quot;Microagent for determining the relevance of the generated response&quot;&quot;&quot;&#10;&#10;    def __init__(self, llm_client: LLMClient):&#10;        self.llm_client = llm_client&#10;        self.system_prompt = &quot;&quot;&quot;&#10;        You are a quality assurance specialist for Amazon product support.&#10;        Given a question, and an answer your task is to verify if a generated answer is relevant, accurate, and helpful.&#10;&#10;        Evaluate the answer based on:&#10;        1. Does it directly address the user's question?&#10;        2. Is it based on the provided information?&#10;        3. Is it helpful and informative?&#10;        4. Does it avoid speculation or generic responses?&#10;        5. Is it written in a clear, professional tone?&#10;&#10;        Respond with only &quot;YES&quot; if the answer is good, or &quot;NO&quot; if it needs improvement.&#10;        &quot;&quot;&quot;&#10;&#10;    def assess_relevance(self, user_question: str, generated_response: str, context: List[List[str]]) -&gt; (bool, str):&#10;        &quot;&quot;&quot;&#10;        Assesses the relevance of the generated response to the user question.&#10;&#10;        Parameters:&#10;        - user_question: str; The question posed by the user.&#10;        - generated_response: str; The response generated by the model.&#10;        - context: List[str]; The context information retrieved for the product.&#10;&#10;        Returns:&#10;        - tuple: (bool, str); True if the generated response is relevant, False otherwise, along with a reason.&#10;        &quot;&quot;&quot;&#10;&#10;        complete_human_input = clean_string(f&quot;&quot;&quot;&#10;        Based on the following information, determine if the generated answer is relevant and helpful.&#10;&#10;        Original question: {user_question}&#10;&#10;        Context: {context}&#10;&#10;        Generated answer: {generated_response}&#10;&#10;        Provide a simple &quot;YES&quot; or &quot;NO&quot; response based on the answer's relevance and helpfulness.&#10;        Do not provide any additional explanations or details other than your &quot;YES&quot; or &quot;NO&quot; answer.&#10;        &quot;&quot;&quot;)&#10;&#10;        messages = [&#10;            {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: self.system_prompt},&#10;            {&quot;role&quot;: &quot;human&quot;, &quot;content&quot;: complete_human_input}&#10;        ]&#10;&#10;        response = self.llm_client.generate_response(messages=messages)&#10;&#10;        return response.strip().upper() == &quot;YES&quot; or response.strip().upper().startswith(&quot;YES&quot;)&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;This module defines the IsRelevantAgent microagent,&#10;which is responsible for determining the relevance of the generated response&#10;to the user's question.&#10;&quot;&quot;&quot;&#10;&#10;from typing import List&#10;&#10;from agent.llm_client import LLMClient&#10;from utils.utils import clean_string&#10;&#10;class IsRelevantAgent:&#10;    &quot;&quot;&quot;Microagent for determining the relevance of the generated response&quot;&quot;&quot;&#10;&#10;    def __init__(self, llm_client: LLMClient):&#10;        self.llm_client = llm_client&#10;        self.system_prompt = &quot;&quot;&quot;&#10;        You are a quality assurance specialist for Amazon product support.&#10;        Given a question, and an answer your task is to verify if a generated answer is relevant, accurate, and helpful.&#10;&#10;        Evaluate the answer based on:&#10;        1. Does it directly address the user's question?&#10;        2. Is it based on the provided information?&#10;        3. Is it helpful and informative?&#10;        4. Does it avoid speculation or generic responses?&#10;        5. Is it written in a clear, professional tone?&#10;&#10;        Respond with only &quot;YES&quot; if the answer is good, or &quot;NO&quot; if it needs improvement.&#10;        &quot;&quot;&quot;&#10;&#10;    def assess_relevance(self, user_question: str, generated_response: str, context: List[List[str]]) -&gt; (bool, str):&#10;        &quot;&quot;&quot;&#10;        Assesses the relevance of the generated response to the user question.&#10;&#10;        Parameters:&#10;        - user_question: str; The question posed by the user.&#10;        - generated_response: str; The response generated by the model.&#10;        - context: List[str]; The context information retrieved for the product.&#10;&#10;        Returns:&#10;        - tuple: (bool, str); True if the generated response is relevant, False otherwise, along with a reason.&#10;        &quot;&quot;&quot;&#10;&#10;        complete_human_input = clean_string(f&quot;&quot;&quot;&#10;        Based on the following information, determine if the generated answer is relevant and helpful.&#10;&#10;        Original question: {user_question}&#10;&#10;        Context: {context}&#10;&#10;        Generated answer: {generated_response}&#10;&#10;        Provide a simple &quot;YES&quot; or &quot;NO&quot; response based on the answer's relevance and helpfulness.&#10;        Do not provide any additional explanations or details other than your &quot;YES&quot; or &quot;NO&quot; answer.&#10;        &quot;&quot;&quot;)&#10;&#10;        messages = [&#10;            {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: self.system_prompt},&#10;            {&quot;role&quot;: &quot;human&quot;, &quot;content&quot;: complete_human_input}&#10;        ]&#10;&#10;        try:&#10;            response = self.llm_client.generate_response(messages=messages).strip().upper()&#10;            if response.startswith(&quot;YES&quot;):&#10;                return True, &quot;The generated response is relevant and helpful.&quot;&#10;            elif response.startswith(&quot;NO&quot;):&#10;                return False, &quot;The generated response is not relevant or helpful.&quot;&#10;            else:&#10;                return False, f&quot;Unexpected response from LLM: {response}&quot;&#10;        except Exception as e:&#10;            return False, f&quot;Error during relevance assessment: {e}&quot;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/streamlit_app.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/streamlit_app.py" />
              <option name="originalContent" value="&quot;&quot;&quot;&#10;Streamlit module for the Alexupport Agent&#10;&quot;&quot;&quot;&#10;&#10;import time&#10;import os&#10;import streamlit as st&#10;from qdrant_client import QdrantClient&#10;from dotenv import load_dotenv&#10;&#10;load_dotenv()&#10;&#10;# Validate environment variables&#10;QDRANT_URL = os.getenv(&quot;QDRANT_URL&quot;)&#10;QDRANT_API_KEY = os.getenv(&quot;QDRANT_API_KEY&quot;)&#10;COLLECTION_NAME = os.getenv(&quot;QDRANT_COLLECTION&quot;)&#10;VECTOR_NAME = os.getenv(&quot;QDRANT_VECTOR_NAME&quot;)&#10;&#10;if not all([QDRANT_URL, QDRANT_API_KEY, COLLECTION_NAME, VECTOR_NAME]):&#10;    st.error(&quot;Missing required environment variables. Please check your .env file.&quot;)&#10;    st.stop()&#10;&#10;# Initialize Qdrant client&#10;try:&#10;    client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)&#10;except Exception as e:&#10;    st.error(f&quot;Failed to initialize Qdrant client: {e}&quot;)&#10;    st.stop()&#10;&#10;from agent.alexupport_agent import AlexupportAgent&#10;from agent.information_retriever import InformationRetriever, client&#10;from agent.llm_client import LLMClient&#10;&#10;LLM_CLIENT = LLMClient()&#10;&#10;def typing_stream(text):&#10;    &quot;&quot;&quot;Simulate typing effect&quot;&quot;&quot;&#10;    for word in text.split():&#10;        yield word + &quot; &quot;&#10;        time.sleep(0.05)&#10;&#10;# Get all points (limit to 2,000 for performance)&#10;points = client.scroll(collection_name=COLLECTION_NAME, limit=2000)[0]&#10;&#10;def main():&#10;    st.set_page_config(page_title=&quot;Alexupport&quot;, page_icon=&quot;&quot;, layout=&quot;wide&quot;)&#10;    st.title(&quot; Alexupport — Amazon Product Support Assistant&quot;)&#10;&#10;    with st.sidebar:&#10;        st.header(&quot;Product&quot;)&#10;        try:&#10;            retriever = InformationRetriever(&#10;                qdrant_client=client,&#10;                llm_client=LLM_CLIENT&#10;            )&#10;            products = retriever.list_products(limit=400)&#10;        except Exception as e:&#10;            st.error(f&quot;Error retrieving products: {e}&quot;)&#10;            st.stop()&#10;&#10;        if not products:&#10;            st.info(&quot;No products found in the collection.&quot;)&#10;            st.stop()&#10;&#10;&#10;        options = [f&quot;{p['asin']} — {p.get('productTitle', '(untitled)')}&quot; for p in products]&#10;        chosen = st.selectbox(&quot;Product&quot;, options, index=0)&#10;        selected_asin = chosen.split(&quot; — &quot;, 1)[0]&#10;        product_title = next((p.get(&quot;productTitle&quot;) for p in products if p[&quot;asin&quot;] == selected_asin), None)&#10;        if selected_asin:&#10;            st.caption(f&quot;[Open on Amazon](https://www.amazon.com/dp/{selected_asin})&quot;)&#10;&#10;    if &quot;agent&quot; not in st.session_state:&#10;        st.session_state[&quot;agent&quot;] = AlexupportAgent()&#10;    agent = st.session_state[&quot;agent&quot;]&#10;&#10;    # ---------- Chat state reset when ASIN changes ----------&#10;    if &quot;asin&quot; not in st.session_state or st.session_state[&quot;asin&quot;] != selected_asin:&#10;        st.session_state[&quot;asin&quot;] = selected_asin&#10;        st.session_state[&quot;messages&quot;] = []&#10;&#10;        # clear conversation memory if your agent exposes it&#10;        try:&#10;            agent.memory.clear()  # ok if you use ConversationBufferMemory&#10;        except Exception:&#10;            pass&#10;&#10;        # use your agent's intro method&#10;        agent.intro() if hasattr(agent, &quot;intro&quot;) else f&quot;Hi! Ask me about {product_title or 'this product'}.&quot;&#10;&#10;&#10;    if &quot;messages&quot; not in st.session_state:&#10;        st.session_state.messages = [{&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: agent.intro()}]&#10;&#10;    # ---------- Display chat history ----------&#10;    for message in st.session_state.messages:&#10;        with st.chat_message(message[&quot;role&quot;]):&#10;            st.markdown(message[&quot;content&quot;])&#10;&#10;    if prompt := st.chat_input(&quot;Ask about the product...&quot;):&#10;        st.session_state.messages.append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt})&#10;        with st.chat_message(&quot;user&quot;):&#10;            st.markdown(prompt)&#10;        with st.chat_message(&quot;assistant&quot;):&#10;            try:&#10;                resp = agent.answer(prompt, asin=selected_asin)&#10;            except Exception as e:&#10;                resp = f&quot;Error: {e}&quot;&#10;            st.session_state.messages.append({&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: resp})&#10;            st.write_stream(typing_stream(resp))&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    main()&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Streamlit module for the Alexupport Agent&#10;&quot;&quot;&quot;&#10;&#10;import time&#10;import os&#10;import streamlit as st&#10;from qdrant_client import QdrantClient&#10;from dotenv import load_dotenv&#10;&#10;load_dotenv()&#10;&#10;# Validate environment variables&#10;QDRANT_URL = os.getenv(&quot;QDRANT_URL&quot;)&#10;QDRANT_API_KEY = os.getenv(&quot;QDRANT_API_KEY&quot;)&#10;COLLECTION_NAME = os.getenv(&quot;QDRANT_COLLECTION&quot;)&#10;VECTOR_NAME = os.getenv(&quot;QDRANT_VECTOR_NAME&quot;)&#10;&#10;if not all([QDRANT_URL, QDRANT_API_KEY, COLLECTION_NAME, VECTOR_NAME]):&#10;    st.error(&quot;Missing required environment variables. Please check your .env file.&quot;)&#10;    st.stop()&#10;&#10;# Initialize Qdrant client&#10;try:&#10;    client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)&#10;except Exception as e:&#10;    st.error(f&quot;Failed to initialize Qdrant client: {e}&quot;)&#10;    st.stop()&#10;&#10;from agent.alexupport_agent import AlexupportAgent&#10;from agent.information_retriever import InformationRetriever, client&#10;from agent.llm_client import LLMClient&#10;&#10;LLM_CLIENT = LLMClient()&#10;&#10;def typing_stream(text):&#10;    &quot;&quot;&quot;Simulate typing effect&quot;&quot;&quot;&#10;    for word in text.split():&#10;        yield word + &quot; &quot;&#10;        time.sleep(0.05)&#10;&#10;# Get all points (limit to 2,000 for performance)&#10;points = client.scroll(collection_name=COLLECTION_NAME, limit=2000)[0]&#10;&#10;def main():&#10;    st.set_page_config(page_title=&quot;Alexupport&quot;, page_icon=&quot;&quot;, layout=&quot;wide&quot;)&#10;    st.title(&quot; Alexupport — Amazon Product Support Assistant&quot;)&#10;&#10;    with st.sidebar:&#10;        st.header(&quot;Product&quot;)&#10;        try:&#10;            retriever = InformationRetriever(&#10;                qdrant_client=client,&#10;                llm_client=LLM_CLIENT&#10;            )&#10;            products = retriever.list_products(limit=400)&#10;        except Exception as e:&#10;            st.error(f&quot;Error retrieving products: {e}&quot;)&#10;            st.stop()&#10;&#10;        if not products:&#10;            st.info(&quot;No products found in the collection.&quot;)&#10;            st.stop()&#10;&#10;&#10;        options = [f&quot;{p['asin']} — {p.get('productTitle', '(untitled)')}&quot; for p in products]&#10;        chosen = st.selectbox(&quot;Product&quot;, options, index=0)&#10;        selected_asin = chosen.split(&quot; — &quot;, 1)[0]&#10;        product_title = next((p.get(&quot;productTitle&quot;) for p in products if p[&quot;asin&quot;] == selected_asin), None)&#10;        if selected_asin:&#10;            st.caption(f&quot;[Open on Amazon](https://www.amazon.com/dp/{selected_asin})&quot;)&#10;&#10;    if &quot;agent&quot; not in st.session_state:&#10;        st.session_state[&quot;agent&quot;] = AlexupportAgent()&#10;    agent = st.session_state[&quot;agent&quot;]&#10;&#10;    # ---------- Chat state reset when ASIN changes ----------&#10;    if &quot;asin&quot; not in st.session_state or st.session_state[&quot;asin&quot;] != selected_asin:&#10;        st.session_state[&quot;asin&quot;] = selected_asin&#10;        st.session_state[&quot;messages&quot;] = []&#10;&#10;        # clear conversation memory if your agent exposes it&#10;        try:&#10;            agent.memory.clear()  # ok if you use ConversationBufferMemory&#10;        except Exception:&#10;            pass&#10;&#10;        # use your agent's intro method&#10;        agent.intro() if hasattr(agent, &quot;intro&quot;) else f&quot;Hi! Ask me about {product_title or 'this product'}.&quot;&#10;&#10;&#10;    if &quot;messages&quot; not in st.session_state:&#10;        st.session_state.messages = [{&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: agent.intro()}]&#10;&#10;    # ---------- Display chat history ----------&#10;    for message in st.session_state.messages:&#10;        with st.chat_message(message[&quot;role&quot;]):&#10;            st.markdown(message[&quot;content&quot;])&#10;&#10;    if prompt := st.chat_input(&quot;Ask about the product...&quot;):&#10;        st.session_state.messages.append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt})&#10;        with st.chat_message(&quot;user&quot;):&#10;            st.markdown(prompt)&#10;        with st.chat_message(&quot;assistant&quot;):&#10;            try:&#10;                resp = agent.answer(prompt, asin=selected_asin)&#10;            except Exception as e:&#10;                resp = f&quot;Error: {e}&quot;&#10;            st.session_state.messages.append({&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: resp})&#10;            st.write_stream(typing_stream(resp))&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    main()" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/test_script.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/test_script.py" />
              <option name="updatedContent" value="import os&#10;import sys&#10;import traceback&#10;from qdrant_client import QdrantClient&#10;from dotenv import load_dotenv&#10;from agent.alexupport_agent import AlexupportAgent&#10;from agent.information_retriever import InformationRetriever&#10;from agent.llm_client import LLMClient&#10;&#10;def validate_environment_variables():&#10;    &quot;&quot;&quot;Validate required environment variables.&quot;&quot;&quot;&#10;    required_vars = [&quot;QDRANT_URL&quot;, &quot;QDRANT_API_KEY&quot;, &quot;QDRANT_COLLECTION&quot;, &quot;QDRANT_VECTOR_NAME&quot;]&#10;    missing_vars = [var for var in required_vars if not os.getenv(var)]&#10;    if missing_vars:&#10;        print(f&quot;Error: Missing environment variables: {', '.join(missing_vars)}&quot;)&#10;        sys.exit(1)&#10;    print(&quot;Environment variables validated successfully.&quot;)&#10;&#10;def test_qdrant_connection():&#10;    &quot;&quot;&quot;Test connection to Qdrant.&quot;&quot;&quot;&#10;    try:&#10;        client = QdrantClient(&#10;            url=os.getenv(&quot;QDRANT_URL&quot;),&#10;            api_key=os.getenv(&quot;QDRANT_API_KEY&quot;)&#10;        )&#10;        print(&quot;Qdrant connection successful.&quot;)&#10;        return client&#10;    except Exception as e:&#10;        print(f&quot;Error: Failed to connect to Qdrant: {e}&quot;)&#10;        sys.exit(1)&#10;&#10;def check_data_availability(client):&#10;    &quot;&quot;&quot;Check if the Qdrant collection contains data.&quot;&quot;&quot;&#10;    collection_name = os.getenv(&quot;QDRANT_COLLECTION&quot;)&#10;    try:&#10;        points = client.scroll(collection_name=collection_name, limit=10)[0]&#10;        if not points:&#10;            print(f&quot;Error: No data found in the Qdrant collection '{collection_name}'.&quot;)&#10;            sys.exit(1)&#10;        print(f&quot;Data found in the Qdrant collection '{collection_name}'.&quot;)&#10;    except Exception as e:&#10;        print(f&quot;Error: Failed to retrieve data from Qdrant: {e}&quot;)&#10;        sys.exit(1)&#10;&#10;def simulate_user_interaction(agent, asin):&#10;    &quot;&quot;&quot;Simulate user interaction with the AlexupportAgent.&quot;&quot;&quot;&#10;    try:&#10;        print(&quot;\nSimulating user interaction...&quot;)&#10;        user_query = &quot;What are the pros and cons of this product?&quot;&#10;        print(f&quot;User: {user_query}&quot;)&#10;        response = agent.answer(user_query, asin=asin)&#10;        print(f&quot;Agent: {response}&quot;)&#10;    except Exception as e:&#10;        print(f&quot;Error during user interaction simulation: {e}&quot;)&#10;        traceback.print_exc()&#10;&#10;def main():&#10;    print(&quot;Starting test script...&quot;)&#10;&#10;    # Step 1: Load environment variables&#10;    load_dotenv()&#10;&#10;    # Step 2: Validate environment variables&#10;    validate_environment_variables()&#10;&#10;    # Step 3: Test Qdrant connection&#10;    client = test_qdrant_connection()&#10;&#10;    # Step 4: Check data availability&#10;    check_data_availability(client)&#10;&#10;    # Step 5: Initialize AlexupportAgent&#10;    llm_client = LLMClient()&#10;    agent = AlexupportAgent()&#10;&#10;    # Step 6: Simulate user interaction&#10;    collection_name = os.getenv(&quot;QDRANT_COLLECTION&quot;)&#10;    products = InformationRetriever(client, llm_client).list_products(limit=1)&#10;    if not products:&#10;        print(f&quot;Error: No products found in the Qdrant collection '{collection_name}'.&quot;)&#10;        sys.exit(1)&#10;    asin = products[0][&quot;asin&quot;]&#10;    simulate_user_interaction(agent, asin)&#10;&#10;    print(&quot;\nTest script completed successfully.&quot;)&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    main()&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>